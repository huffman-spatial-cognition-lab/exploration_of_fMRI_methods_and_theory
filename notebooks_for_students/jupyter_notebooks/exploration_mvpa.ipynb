{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate pattern analysis of fMRI data to study category representation in the human brain\n",
    "\n",
    "## In this notebook, we will work through a multivariate pattern analysis (MVPA) of one participant from Haxby et al. (2001)\n",
    "\n",
    "The goal here is for you to learn about how to implement multivariate pattern analysis (MVPA), which attempts to determine whether distributed patterns of activity differ between conditions. In this case, we will be using machine learning to look at patterns of activaty in the visual system in response to different categories of images (e.g., faces, houses). This analysis will be done on a single participant from a landmark study: \n",
    "\n",
    "Haxby, J. V., Gobbini, M. I., Furey, M. L., Ishai, A., Schouten, J. L., & Pietrini, P. (2001). Distributed and overlapping representations of faces and objects in ventral temporal cortex. *Science*, 293, 2425-2430. http://dx.doi.org/10.1126/science.1063736\n",
    "\n",
    "## Why are you having us analyze fMRI data in introductory cognitive neuroscience course?\n",
    "I (Prof. Derek Huffman) believe that we learn best by actually working with the content from the field. For example, if you want to learn how to paint, sure, you can read a book about painting, you can look at paintings, you might even learn from watching videos of other people painting and lectures in which experienced artists teach you how to paint. However, you are going to learn best by actually picking up a paintbrush, dipping it in the paint, and trying to paint something yourself. You will learn how to mix colors together, how much pressure to apply to the canvas, which paintbrush to use in different situations, how long to let paint dry before adding different colors, and (perhaps most importantly) you will learn from your mistakes. Simply put, you will learn a lot more by practicing (as the saying goes: \"experience is the best teacher\"). Moreover, these practical experiences will help you learn more from your other resources. For example, you might revisit readings or discussions to really think about *why* we use certain techniques. Similarly, to *really* understand cognitive neuroscience, I believe that we have to actually practice some cognitive neuroscience, so pick up your paintbrush and start painting some brain maps with the Python code below!\n",
    "\n",
    "## Attributions and citations for this notebook\n",
    "Written by Professor Derek J. Huffman at Colby College, Waterville, ME 04901, USA (with the nilearn tutorials as a helpful starting point; Nilearn Contributors et al., 2023)\n",
    "\n",
    "Most recent updates: 2024 JAN 18\n",
    "\n",
    "Please keep the attribution line above as well as the citations below (update as you change the code):\n",
    "\n",
    "Huffman, D. J. (2024). An in-depth exploration of the interplay between fMRI methods and theory in cognitive neuroscience. PsyArXiv.\n",
    "\n",
    "Nilearn Contributors et al. (2023) nilearn. Available at: https://zenodo.org/record/8397156."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from nilearn.datasets import fetch_haxby\n",
    "from nilearn.image import index_img\n",
    "from nilearn.image import load_img\n",
    "from nilearn.glm.first_level import FirstLevelModel\n",
    "from nilearn.plotting import plot_design_matrix\n",
    "from nilearn.plotting import plot_stat_map\n",
    "from nilearn.plotting import show\n",
    "from nilearn.plotting import view_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the 'from nilearn.glm.first_level import FirstLevelModel' will throw a warning about nilearn.glm being experimental (AKA we are on the cutting edge haha), but we can ignore this for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gather the fMRI and event/behavioral/timing data\n",
    "\n",
    "Next, we will want to get all of the data. Here, we will primarily rely on the fetch_haxby() function. This function will load data from a single participant (subj2) to the location we set for datapath below.\n",
    "\n",
    "#### First, define a variable with the path to the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = '.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Then, use the fetch_haxby function to load the data:\n",
    "Note: here we give the function the input of the datapath to let it know where to find the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_data = fetch_haxby(datapath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can take a look at the subject_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what you might notice above is that for these nilearn datasets, it will essentially load strings that represent the location sof different files. You will note a few files of interest:\n",
    "\n",
    "1) 'anat' = the anatomical / structural image of the participant's brain (T1 MRI)\n",
    "\n",
    "2) 'func' = the functional image of the participant's brain (i.e., fMRI)\n",
    "\n",
    "3) 'mask_vt' = a mask of the ventral temporal lobe, which is important for high-level vision\n",
    "\n",
    "4) 'mask_face' = a mask of voxels that respond to faces (i.e., greater BOLD activity)\n",
    "\n",
    "5) 'mask_house' = a mask of voxels that respond to houses (i.e., greater BOLD activity)\n",
    "\n",
    "6) 'session_target' = the conditions of the dataset (here, labels = frame-by-frame indication of what was happening; chunks = the run or scan number from which that frame came... fMRI are gathered in runs, but the func file above is the concatenation or combination of all of those runs together).\n",
    "\n",
    "In this particular case (for these nilearn datasets), the functional data can be accessed like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmri = subject_data.func[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gather the labels for each frame. This information will be necessary for knowing what was happening on each frame of the experiment. The 'labels' column contains the task condition of each frame and the 'chunks' column contains the run information. These will be key features for training and testing the classifier (see below).\n",
    "\n",
    "To load this file, we will use the pandas function read_csv. Note: a CSV file (comma-separated values) is one way of saving data in a file where things are separated by commas. Here, the file is separated by blank spaces, hence the sep=' ' (i.e., notice the blank space between the quotes). This function will create a pandas DataFrame, which is a convenient way of storing this type of data. The DataFrame is similar to a numpy.array, but allows for data with different types in different columns. Similar to a dictionary, you can access the columns with the following syntax, e.g., tr_labels['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_info = pd.read_csv(subject_data.session_target[0], sep=' ')\n",
    "tr_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we need to set up the decoder (yes, machine learning, here we come!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.decoding import Decoder\n",
    "from sklearn.model_selection import LeaveOneGroupOut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up a function for printing the classification results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_class_accuracy(d):\n",
    "    \"\"\"\n",
    "    A function for printing the classification results.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    d : a fit decoder\n",
    "        A model that was fit via nilearn\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Out : None\n",
    "        Prints the classification accuracy and chance level\n",
    "    \"\"\"\n",
    "    classification_accuracy = np.mean(list(d.cv_scores_.values()))\n",
    "    chance_level = 1. / len(d.classes_)\n",
    "    print('Classifying {}'.format(d.classes_))\n",
    "    print('Classification accuracy: {:.4f} / Chance level: {}'.format(\n",
    "        classification_accuracy, chance_level))\n",
    "    return classification_accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will start by looking at the classification of faces vs. houses (i.e., related to our last assignment in that employed an activation analysis).\n",
    "\n",
    "We will start by training and testing the classifier on individual frames of fMRI data. That is, we have 121 frames per run. Each block of trials is 24 seconds long, so we will be using 9 frames from each of those blocks per condition per run. For example, here you can see the labels for scissors for the first run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tr_info['labels'][0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_mask = tr_info['labels'].isin(['face', 'house'])\n",
    "conditions_sel = tr_info['labels'][condition_mask]\n",
    "run_label = tr_info['chunks'][condition_mask]\n",
    "fmri_sel = index_img(fmri, condition_mask)\n",
    "\n",
    "# Confirm that we now have 2 conditions\n",
    "print(conditions_sel.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And let's see how well a classifier can distinguish trials in which participants are viewing faces vs. trials in which they are viewing houses.\n",
    "\n",
    "#### Some notes about the classifier\n",
    "\n",
    "1) Here, we will use a support vector machine (SVM; estimator='svc'). The SVM is commonly used in fMRI because we have a lot of features (i.e., voxels) but a limited number of trials. Some machine learning classifiers will suffer from overfitting (i.e., not generalizing well to new data) because they have a lot of free parameters. The SVM does not change its number of parameters with more features, therefore it is a good technique for fMRI. More specifically, we will be using a linear SVM, which also has benefits with respect to interpretting the results. Typically, in the case of a linear classifier, we would say that if a region can classify between categories of stimuli, then it contains information about that category. This is a bit beyond the scope of what you need to know for this class, but with a nonlinear classifier, it is more difficult to know whether or not that region \"contains the representation\" (vs. other downstream regions, which would be able to \"read this information out\").\n",
    "\n",
    "2) We will use an approach called *cross-validation*. This is a common approach in machine learning and it is the process of partitioning your data into a \"training set\" and a \"test set.\" The basic idea is that you want to train and test your classifier on indepdent data to see how well it can generalize to new examples. Our measure of interest is the generalization performance on the test data. One thing to note in the context of fMRI data is that timepoints that come from the same run are *not* independent. For example, the slow timing of the hemodynamic response function makes nearby timepoints more similar to each other (i.e., the are correlated so they are not indepdendent). In a nutshell, it is best practice to train and test the classifier on data from different runs. So, here, we employ what is called leave-one-run-out cross-validation, where we train and test our classifier on data from each run. For example, on the first iteration, we would train our classifier on the data from runs 1 to 11 and then test the classifier on the data from run 0. The next iteration we would train the classifier on runs 0 and 2 to 11 and then test it on run 1, and so forth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = LeaveOneGroupOut()\n",
    "decoder = Decoder(estimator='svc', mask=subject_data.mask_vt[0], \n",
    "                  standardize='zscore_sample', screening_percentile=100, \n",
    "                  scoring='accuracy', cv=cv)\n",
    "\n",
    "# Compute the prediction accuracy for the different folds (i.e. session)\n",
    "decoder.fit(fmri_sel, conditions_sel, groups=run_label)\n",
    "\n",
    "decoder.cv_scores_['face']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What you see above is the classification accuracy for each of the validation runs. Notice that there are 12 accuracy scores here? These correspond to the 12 fMRI runs! We can calculate the average classification accuracy across all of these cross-validations, which will give an overall measure of accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_trs_vtc = print_class_accuracy(decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, I think that is pretty impressive! What do you think? (Remember, this is classifying single frames/images of fMRI data!)\n",
    "\n",
    "\n",
    "### Now, let's combine the code above into a function so that we can re-use this framework\n",
    "\n",
    "It is generally better to us a function rather than copying and pasting the same code all over the place. For example, there is a smaller chance of error and it is more (human) readable. So, here let's get the pieces above into a workable function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_data(fmri_str, behavioral, conditions_arr):\n",
    "    \"\"\"\n",
    "    Function for selecting data from a given conditions array.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    fmri_str : string\n",
    "        A string indicating the path and name of the fMRI data\n",
    "    behavioral : pandas.DataFrame\n",
    "        The behavioral conditions (here, expecting the Haxby data\n",
    "        with the colums of labels and chunks)\n",
    "    conditions_arr : a numpy.array\n",
    "        A 1D numpy.array with the conditions to include\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    fmri_sel : the fMRI data from the selected frames\n",
    "    conditions_sel : a numpy.array with the conditions labels\n",
    "    run_sel : a numpy.array with the run labels\n",
    "    \"\"\"\n",
    "    condition_mask = behavioral['labels'].isin(conditions_arr)\n",
    "    conditions_sel = behavioral['labels'][condition_mask]\n",
    "    run_sel = behavioral['chunks'][condition_mask]\n",
    "    fmri_sel = index_img(fmri_str, condition_mask)\n",
    "    return fmri_sel, conditions_sel, run_sel\n",
    "\n",
    "\n",
    "def svm_loro_cv(fmri_data, behav_df, mask, conditions_arr):\n",
    "    \"\"\"\n",
    "    Wrapper for running LORO cross validation with an SVM.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    fmri_data : string\n",
    "        A string indicating the path and name of the fMRI data\n",
    "    behav_df : pandas.DataFrame\n",
    "        The behavioral conditions (here, expecting the Haxby data\n",
    "        with the columns of labels and chunks)\n",
    "    conditions_arr : a numpy.array\n",
    "        A 1D numpy.array with the conditions to include\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Out : decoder\n",
    "        Prints the decoding results and returns the fit nilearn decoder.\n",
    "    \"\"\"\n",
    "    cv = LeaveOneGroupOut()\n",
    "    fmri_sel, cond_sel, run_sel = select_data(fmri_data, behav_df, \n",
    "                                              conditions_arr)\n",
    "    decoder = Decoder(estimator='svc', mask=mask, standardize='zscore_sample', \n",
    "                      screening_percentile=100, scoring='accuracy', cv=cv)\n",
    "    decoder.fit(fmri_sel, cond_sel, groups=run_sel)\n",
    "    avg_class_accuracy = print_class_accuracy(decoder)\n",
    "    return decoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify this gives us the same exact values as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = svm_loro_cv(fmri, tr_info, subject_data.mask_vt[0], np.array(('face', 'house')))\n",
    "res.cv_scores_['face']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Looks like it worked!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, we will want to see what happens if we restrict our analysis to specific regions of interest\n",
    "\n",
    "#### How about looking at the house-selective voxels?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = svm_loro_cv(fmri, tr_info, subject_data.mask_house[0], \n",
    "                  np.array(('face', 'house')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quite similar to the overall accuracy throughout the ventral temporal cortex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, we will want to look at other categories: how selective is the \"place\" area?\n",
    "\n",
    "Here, we will look at how well this region can classify other categories of stimuli: cat vs. shoe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = svm_loro_cv(fmri, tr_info, subject_data.mask_house[0], \n",
    "                  np.array(('cat', 'shoe')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1\n",
    "\n",
    "What does it mean that the \"house-selective\" region can classify between different categories of visual stimuli?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, let's try running the decoder on the output from a general linear model (GLM)\n",
    "\n",
    "The steps above were fitting / testing the classifier on individual fMRI frames (i.e., TRs; the images that took place during the viewing of images from different categories). However, these could each be relatively noisy and it is generally better to run a GLM to estimate activation to different trials/stimuli/etc. Below, we will slightly modify our code from last week to gather activity in response to each category for each run.\n",
    "\n",
    "### First, let's set up our events DataFrame so that we can fit the models (GLMs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gather the event timing (onsets and durations), which we will use to set up our models (see below). The difference between tr_info (above) and events (below) is that the former contains information about what is happening for each frame of the fMRI data while the latter shows the exact times (in seconds) at which different events happen.\n",
    "\n",
    "Note: the events.csv file in the line below is the file that you uploaded from Moodle into the directory with this notebook. If you get an error on this line, then it probably means that you need to upload this file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = pd.read_csv('%s/events.csv' % datapath)\n",
    "events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract the fMRI and event files into a list where the elements are the runs.\n",
    "\n",
    "Now, we will want to separate the data based on the runs. Here, we will load the data into lists in which each element contains the data for one fMRI run. \n",
    "\n",
    "Note: hopefully the [x for x in y] type of syntax is familar from the earlier homeworks/readings, but this is basically a way that you can populate a list with a for loop in a single line of code. Here, it will loop over all of the runs and separate the data into a list. We will end up with lists with 12 elements: fmri_runs will have 12 runs of fMRI data and events_runs will have 12 runs of events/behavioral/timing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_nums = np.unique(events['run'])\n",
    "\n",
    "fmri_runs = [index_img(fmri, tr_info['chunks'] == run_i) for run_i in run_nums]\n",
    "events_runs = [events[events['run'] == run_i] for run_i in run_nums]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify our code slightly from last week to output z-maps.\n",
    "\n",
    "These files will create one z-score map for each category for each run. That is, instead of having 9 examples of each category per run (as above), we will end up with 1 example of each category per run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drift_model = 'polynomial'\n",
    "poly_drift_order = 3\n",
    "high_pass = 0.01\n",
    "mask_img = subject_data.mask_vt[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up empty lists in which we can store our information of interest --------\n",
    "z_maps = []\n",
    "task_label_glm = []\n",
    "run_label_glm = []\n",
    "\n",
    "# set up an array with all of the unique conditions (for looping below) -------\n",
    "task_unique = np.unique(events['trial_type'])\n",
    "\n",
    "# loop over all of the runs to run the GLM ------------------------------------\n",
    "for run_i in np.arange(len(fmri_runs)):\n",
    "    fmri_glm = FirstLevelModel(t_r=2.5, noise_model='ar1', standardize=False,\n",
    "                               hrf_model='spm', drift_model=drift_model, \n",
    "                               drift_order=poly_drift_order, \n",
    "                               high_pass=high_pass, mask_img=mask_img)\n",
    "    print(fmri_runs[run_i].shape)\n",
    "    print(events_runs[run_i])\n",
    "    fmri_glm = fmri_glm.fit(fmri_runs[run_i], events_runs[run_i])\n",
    "    # extract z-statistics for each condition; i.e., our GLM-based patterns ---\n",
    "    # and append the z-statistics, task labels, and run labels ----------------\n",
    "    for condition_i in task_unique:\n",
    "        print(condition_i)\n",
    "        z_maps.append(fmri_glm.compute_contrast(condition_i))\n",
    "        task_label_glm.append(condition_i)\n",
    "        run_label_glm.append(run_i)\n",
    "    plot_design_matrix(fmri_glm.design_matrices_[0])\n",
    "    plt.show()\n",
    "\n",
    "# convert the lists to numpy arrays -------------------------------------------\n",
    "z_maps = np.array(z_maps)\n",
    "task_label_glm = np.array(task_label_glm)\n",
    "run_label_glm = np.array(run_label_glm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here are a couple of functions that will help for the classification of different categories with different masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_class_zmap(z_maps_arg, task_label_arg, run_label_arg, \n",
    "                   mask, categories):\n",
    "    \"\"\"\n",
    "    Run classification analysis on z-maps with categories.\n",
    "    \n",
    "    Note: here, I am omitting z-scoring because we are already using\n",
    "    the z-maps, thus we are effectively standardized within each run.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    z_maps_arg : numpy.array with z-score maps\n",
    "        A numpy.array with z-score maps from a GLM\n",
    "    task_label_arg : a numpy.array of strings\n",
    "        The task labels that correspond to the z-score maps\n",
    "    run_label_arg : a numpy.array of int\n",
    "        The run labels that correspond to the z-score maps\n",
    "    mask : string\n",
    "        A string indicating the path and name of the mask\n",
    "    categories: numpy.array\n",
    "        A numpy.array of the categories to include in the classification\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Out : None\n",
    "        Does not return anything but prints the results of decoding.\n",
    "    \"\"\"\n",
    "    sel_mask = [x in categories for x in task_label_arg]\n",
    "    z_maps_sel = z_maps_arg[sel_mask]\n",
    "    task_sel = task_label_arg[sel_mask]\n",
    "    run_sel = run_label_arg[sel_mask]\n",
    "    decoder = Decoder(estimator='svc', mask=mask, standardize=False,\n",
    "                      screening_percentile=100, cv=LeaveOneGroupOut())\n",
    "    decoder.fit(z_maps_sel, task_sel, groups=run_sel)\n",
    "    avg_acc = print_class_accuracy(decoder)\n",
    "    return avg_acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then, let's run the classification for faces vs. houses in VT with these z-score maps (from the GLMs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_FvH_vtc = run_class_zmap(z_maps, task_label_glm, run_label_glm, \n",
    "                             mask=subject_data.mask_vt[0], \n",
    "                             categories=np.array(['face', 'house']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, 100% now! \n",
    "\n",
    "### What about classification in VT for other CATegories?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_CvS_vtc = run_class_zmap(z_maps, task_label_glm, run_label_glm,\n",
    "                             mask=subject_data.mask_vt[0], \n",
    "                             categories=np.array(['cat', 'shoe']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is also looking purrfect! ;) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try the same process for the \"house\" selective voxels\n",
    "\n",
    "#### Faces vs. houses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_FvH_ppa = run_class_zmap(z_maps, task_label_glm, run_label_glm, \n",
    "                             mask=subject_data.mask_house[0], \n",
    "                             categories=np.array(['face', 'house']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cats vs. shoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT YOUR CODE HERE\n",
    "res_CvS_ppa = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try the same process for the \"face\" selective voxels\n",
    "\n",
    "#### Faces vs. houses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_FvH_ffa = run_class_zmap(z_maps, task_label_glm, run_label_glm, \n",
    "                             mask=subject_data.mask_face[0], \n",
    "                             categories=np.array(['face', 'house']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cats vs. shoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT YOUR CODE HERE\n",
    "res_CvS_ffa = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's create a figure with the results for the classification of Faces vs. \"Places\" and Cats vs. Shoes in the ventral temporal cortex as well as the face-selective voxels and the \"place\"-selective voxels\n",
    "\n",
    "#### First, we need to get the data into a pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all of the classification accuracy into a numpy array -------------------\n",
    "# here, we will multiply by 100 since percent accuracy is easier for plots ----\n",
    "acc_all_data = np.array((res_FvH_vtc, res_FvH_ffa, res_FvH_ppa, \n",
    "                         res_CvS_vtc, res_CvS_ffa, res_CvS_ppa)) * 100\n",
    "\n",
    "# create a numpy array with the classification categories ---------------------\n",
    "classification_categories = np.append(np.repeat(\"Faces vs. Places\", 3), \n",
    "                                      np.repeat(\"Cats vs. Shoes\", 3))\n",
    "\n",
    "# create a numpy array with the region of interest information (i.e., mask) ---\n",
    "region_of_interest = np.array((\"VTC\", \"FFA\", \"PPA\", \"VTC\", \"FFA\", \"PPA\"))\n",
    "\n",
    "# place it all into a DataFrame for easy plotting in seaborn ------------------\n",
    "df = pd.DataFrame({\"Classification accuracy\" : acc_all_data, \n",
    "                   \"Classification categories\" : classification_categories, \n",
    "                   \"Region of interest\" : region_of_interest})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, we can plot the data using seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(df, col=\"Classification categories\", hue=\"Region of interest\", \n",
    "                  palette=\"colorblind\", aspect=0.65)\n",
    "g.map(sns.barplot, \"Region of interest\", \"Classification accuracy\", \n",
    "      order=[\"VTC\", \"FFA\", \"PPA\"])\n",
    "g.set_titles('{col_name}')\n",
    "plt.savefig(\"classification_glm.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2\n",
    "\n",
    "How do the results from this GLM-based approach compare to the single-TR based approach above? For example, is classification accuracy generally better/worse/the same?\n",
    "\n",
    "#### Question 3\n",
    "\n",
    "If there were differences between single TRs and the GLM, what could potentially cause these differences?\n",
    "\n",
    "#### Question 4\n",
    "\n",
    "How do you interpret these findings? How do these results fit in with the activation analysis from last week? For example, similar to Question 1 above, do these results cause you to reinterpret your conclusions from last week (e.g., about potential modularity / specialization)? Why or why not? Please also discuss which theory or theories activation analysis vs. MVPA allow us to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
